cls
cls()
install.packages("Rfacebook")
install.packages("RCurl")
help("Rfacebook")
help(Rfacebook)
??Rfacebook
cls
clear
setwd("~/")
setwd("~/")
q()
setwd("E:/R Machine Learning/Section 6")
load("E:/R Machine Learning/Section 6/.RData")
View(credit.df)
View(myapp)
load("E:/R Machine Learning/Section 6/.RData")
View(plot.roc.curve)
plot.roc.curve()
library(ROCR)
plot.roc.curve <- function(predictions, title.text){
perf <- performance(predictions, "tpr", "fpr")
plot(perf,col="black",lty=1, lwd=2,
main=title.text, cex.main=0.6, cex.lab=0.8,xaxs="i", yaxs="i")
abline(0,1, col="red")
auc <- performance(predictions,"auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc,2)
legend(0.4,0.4,legend=c(paste0("AUC: ",auc)),cex=0.6,bty = "n",box.col = "white")
}
library(rpart)# tree models
library(caret) # feature selection
library(rpart.plot) # plot dtree
library(ROCR) # model evaluation
library(e1071) # tuning model
source("performance_plot_utils.R") # plotting curves
test.feature.vars <- test.data[,-1]
test.class.var <- test.data[,1]
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
dt.model <- rpart(formula=formula.init, method="class",data=train.data,
control = rpart.control(minsplit=20, cp=0.05))
dt.predictions <- predict(dt.model, test.feature.vars, type="class")
u= union(dt.predictions,test.class.var)
confusionMatrix(table(factor(dt.predictions, u),
factor(test.class.var, u)))
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
control <- trainControl(method="repeatedcv", number=10, repeats=2)
model <- train(formula.init, data=train.data, method="rpart",
trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance, cex.lab=0.5)
formula.new <- "credit.rating ~ account.balance + savings +
credit.amount + credit.duration.months +
previous.credit.payment.status"
formula.new <- as.formula(formula.new)
dt.model.new <- rpart(formula=formula.new, method="class",
data=train.data,
control = rpart.control(minsplit=20, cp=0.05),
parms = list(prior = c(0.7, 0.3)))
dt.predictions.new <- predict(dt.model.new, test.feature.vars,
type="class")
u= union(dt.predictions.new ,test.class.var)
confusionMatrix(table(factor(dt.predictions.new, u),
factor(test.class.var, u)))
dt.model.best <- dt.model.new
print(dt.model.best)
par(mfrow=c(1,1))
prp(dt.model.best, type=1, extra=3, varlen=0, faclen=0)
dt.predictions.best <- predict(dt.model.best, test.feature.vars,
type="prob")
dt.prediction.values <- dt.predictions.best[,2]
predictions <- prediction(dt.prediction.values, test.class.var)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text="DT ROC Curve")
plot.pr.curve(predictions, title.text="DT Precision/Recall Curve")
dt.model.best <- dt.model.new
print(dt.model.best)
par(mfrow=c(1,1))
prp(dt.model.best, type=1, extra=3, varlen=0, faclen=0)
dt.predictions.best <- predict(dt.model.best, test.feature.vars,
type="prob")
dt.prediction.values <- dt.predictions.best[,2]
predictions <- prediction(dt.prediction.values, test.class.var)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text="DT ROC Curve")
plot.pr.curve(predictions, title.text="DT Precision/Recall Curve")
auc<- performance(predictions,"auc")
auc<- unlist(slot(auc,"y.values"))
auc<- round(auc,4)
auc
library(randomForest) #rf model
library(caret) # feature selection
library(e1071) # model tuning
library(ROCR) # model evaluation
source("performance_plot_utils.R") # plot curves
test.feature.vars <- test.data[,-1]
test.class.var <- test.data[,1]
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
rf.model <- randomForest(formula.init, data = train.data,
importance=T, proximity=T)
print(rf.model)
rf.predictions <- predict(rf.model, test.feature.vars, type="class")
u= union(rf.predictions,test.class.var)
confusionMatrix(table(factor(rf.predictions, u),
factor(test.class.var, u)))
formula.new <- "credit.rating ~ account.balance + savings +
credit.amount + credit.duration.months +
previous.credit.payment.status"
formula.new <- as.formula(formula.new)
rf.model.new <- randomForest(formula.new, data = train.data,
importance=T, proximity=T)
rf.predictions.new <- predict(rf.model.new, test.feature.vars,
type="class")
u= union(rf.predictions.new, test.class.var)
confusionMatrix(table(factor(rf.predictions.new, u),
factor(test.class.var, u)))
nodesize.vals <- c(2, 3, 4, 5)
ntree.vals <- c(200, 500, 1000, 2000)
tuning.results <- tune.randomForest(formula.new,
data = train.data,
mtry=3,
nodesize=nodesize.vals,
ntree=ntree.vals)
print(tuning.results)
rf.model.best <- tuning.results$best.model
rf.predictions.best <- predict(rf.model.best, test.feature.vars,
type="class")
u= union(rf.predictions.best,test.class.var)
confusionMatrix(table(factor(rf.predictions.best, u),
factor(test.class.var, u)))
rf.predictions.best <- predict(rf.model.best, test.feature.vars,
type="prob")
rf.prediction.values <- rf.predictions.best[,2]
predictions <- prediction(rf.prediction.values, test.class.var)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text="RF ROC Curve")
plot.pr.curve(predictions, title.text="RF Precision/Recall Curve")
auc<- performance(predictions,"auc")
auc<- unlist(slot(auc,"y.values"))
auc<- round(auc,4)
auc
library(caret) # nn models
library(ROCR) # evaluate models
source("performance_plot_utils.R") # plot curves
test.feature.vars <- test.data[,-1]
test.class.var <- test.data[,1]
transformed.train <- train.data
transformed.test <- test.data
for (variable in categorical.vars){
new.train.var <- make.names(train.data[[variable]])
transformed.train[[variable]] <- new.train.var
new.test.var <- make.names(test.data[[variable]])
transformed.test[[variable]] <- new.test.var
}
transformed.train <- to.factors(df=transformed.train, variables=categorical.vars)
transformed.test <- to.factors(df=transformed.test, variables=categorical.vars)
transformed.test.feature.vars <- transformed.test[,-1]
transformed.test.class.var <- transformed.test[,1]
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
nn.model <- train(formula.init, data = transformed.train,
method="nnet")
print(nn.model)
nn.predictions <- predict(nn.model, transformed.test.feature.vars,
type="raw")
u= union(nn.predictions, transformed.test.class.var)
confusionMatrix(table(factor(nn.predictions, u),
factor(transformed.test.class.var, u)))
formula.init <- "credit.rating ~ ."
formula.init <- as.formula(formula.init)
control <- trainControl(method="repeatedcv", number=10, repeats=2)
model <- train(formula.init, data=transformed.train, method="nnet",
trControl=control)
importance <- varImp(model, scale=FALSE)
plot(importance, cex.lab=0.5)
formula.new <- "credit.rating ~ account.balance + credit.purpose +
savings + current.assets +foreign.worker +
previous.credit.payment.status"
formula.new <- as.formula(formula.new)
nn.model.new <- train(formula.new, data=transformed.train,
method="nnet")
nn.predictions.new <- predict(nn.model.new, transformed.test.feature.vars,
type="raw")
u= union(nn.predictions.new, transformed.test.class.var)
confusionMatrix(table(factor(nn.predictions.new, u),
factor(transformed.test.class.var, u)))
plot(nn.model.new, cex.lab=0.5)
nn.model.best <- nn.model
nn.predictions.best <-predict(nn.model.best,transformed.test.feature.vars,
type="prob")
nn.prediction.values <- nn.predictions.best[,2]
predictions <- prediction(nn.prediction.values, test.class.var)
par(mfrow=c(1,2))
plot.roc.curve(predictions, title.text="NN ROC Curve")
plot.pr.curve(predictions, title.text="NN Precision/Recall Curve")
auc<- performance(predictions,"auc")
auc<- unlist(slot(auc,"y.values"))
auc<- round(auc,4)
auc
